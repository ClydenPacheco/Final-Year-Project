{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 10959,
     "status": "ok",
     "timestamp": 1718822278029,
     "user": {
      "displayName": "Clyden Pacheco",
      "userId": "07798596330758231105"
     },
     "user_tz": -330
    },
    "id": "6mzi5zAb4CwW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Clyden Pacheco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1337,
     "status": "ok",
     "timestamp": 1718822281151,
     "user": {
      "displayName": "Clyden Pacheco",
      "userId": "07798596330758231105"
     },
     "user_tz": -330
    },
    "id": "BbFxE9sU4kWy"
   },
   "outputs": [],
   "source": [
    "df_for_training = pd.read_csv(\"C:/Users/Clyden Pacheco/Documents/Excel/df_for_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1718822282752,
     "user": {
      "displayName": "Clyden Pacheco",
      "userId": "07798596330758231105"
     },
     "user_tz": -330
    },
    "id": "c3KghI2q4tgN",
    "outputId": "8b3fce72-47e7-49d8-90fd-d0ea27ec2995"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24548, 57)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Sentiments</th>\n",
       "      <th>Stock_ADANIENT.NS</th>\n",
       "      <th>Stock_ADANIPORTS.NS</th>\n",
       "      <th>Stock_APOLLOHOSP.NS</th>\n",
       "      <th>...</th>\n",
       "      <th>Stock_SHRIRAMFIN.NS</th>\n",
       "      <th>Stock_SUNPHARMA.NS</th>\n",
       "      <th>Stock_TATACONSUM.NS</th>\n",
       "      <th>Stock_TATAMOTORS.NS</th>\n",
       "      <th>Stock_TATASTEEL.NS</th>\n",
       "      <th>Stock_TCS.NS</th>\n",
       "      <th>Stock_TECHM.NS</th>\n",
       "      <th>Stock_TITAN.NS</th>\n",
       "      <th>Stock_ULTRACEMCO.NS</th>\n",
       "      <th>Stock_WIPRO.NS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344884</td>\n",
       "      <td>0.313330</td>\n",
       "      <td>0.378343</td>\n",
       "      <td>0.339727</td>\n",
       "      <td>0.339412</td>\n",
       "      <td>0.024234</td>\n",
       "      <td>0.454308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.339109</td>\n",
       "      <td>0.298554</td>\n",
       "      <td>0.344151</td>\n",
       "      <td>0.298220</td>\n",
       "      <td>0.297923</td>\n",
       "      <td>0.034739</td>\n",
       "      <td>0.454308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.372607</td>\n",
       "      <td>0.333843</td>\n",
       "      <td>0.339674</td>\n",
       "      <td>0.337489</td>\n",
       "      <td>0.337175</td>\n",
       "      <td>0.143612</td>\n",
       "      <td>0.454308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.354257</td>\n",
       "      <td>0.314964</td>\n",
       "      <td>0.379393</td>\n",
       "      <td>0.333502</td>\n",
       "      <td>0.333189</td>\n",
       "      <td>0.024412</td>\n",
       "      <td>0.454308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.349835</td>\n",
       "      <td>0.314130</td>\n",
       "      <td>0.347202</td>\n",
       "      <td>0.299314</td>\n",
       "      <td>0.299016</td>\n",
       "      <td>0.043164</td>\n",
       "      <td>0.454308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24543</th>\n",
       "      <td>0.490687</td>\n",
       "      <td>0.485012</td>\n",
       "      <td>0.493933</td>\n",
       "      <td>0.470557</td>\n",
       "      <td>0.472697</td>\n",
       "      <td>0.044699</td>\n",
       "      <td>0.256683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24544</th>\n",
       "      <td>0.467802</td>\n",
       "      <td>0.459897</td>\n",
       "      <td>0.358522</td>\n",
       "      <td>0.438704</td>\n",
       "      <td>0.440973</td>\n",
       "      <td>0.115144</td>\n",
       "      <td>0.166468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24545</th>\n",
       "      <td>0.434806</td>\n",
       "      <td>0.509047</td>\n",
       "      <td>0.480143</td>\n",
       "      <td>0.510171</td>\n",
       "      <td>0.512151</td>\n",
       "      <td>0.071241</td>\n",
       "      <td>0.367750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24546</th>\n",
       "      <td>0.515966</td>\n",
       "      <td>0.546854</td>\n",
       "      <td>0.554881</td>\n",
       "      <td>0.561028</td>\n",
       "      <td>0.562802</td>\n",
       "      <td>0.096620</td>\n",
       "      <td>0.533555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24547</th>\n",
       "      <td>0.596062</td>\n",
       "      <td>0.678639</td>\n",
       "      <td>0.641478</td>\n",
       "      <td>0.687098</td>\n",
       "      <td>0.688363</td>\n",
       "      <td>0.556647</td>\n",
       "      <td>0.633596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24548 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Open      High       Low     Close  Adj Close    Volume  \\\n",
       "0      0.344884  0.313330  0.378343  0.339727   0.339412  0.024234   \n",
       "1      0.339109  0.298554  0.344151  0.298220   0.297923  0.034739   \n",
       "2      0.372607  0.333843  0.339674  0.337489   0.337175  0.143612   \n",
       "3      0.354257  0.314964  0.379393  0.333502   0.333189  0.024412   \n",
       "4      0.349835  0.314130  0.347202  0.299314   0.299016  0.043164   \n",
       "...         ...       ...       ...       ...        ...       ...   \n",
       "24543  0.490687  0.485012  0.493933  0.470557   0.472697  0.044699   \n",
       "24544  0.467802  0.459897  0.358522  0.438704   0.440973  0.115144   \n",
       "24545  0.434806  0.509047  0.480143  0.510171   0.512151  0.071241   \n",
       "24546  0.515966  0.546854  0.554881  0.561028   0.562802  0.096620   \n",
       "24547  0.596062  0.678639  0.641478  0.687098   0.688363  0.556647   \n",
       "\n",
       "       Sentiments  Stock_ADANIENT.NS  Stock_ADANIPORTS.NS  \\\n",
       "0        0.454308                1.0                  0.0   \n",
       "1        0.454308                1.0                  0.0   \n",
       "2        0.454308                1.0                  0.0   \n",
       "3        0.454308                1.0                  0.0   \n",
       "4        0.454308                1.0                  0.0   \n",
       "...           ...                ...                  ...   \n",
       "24543    0.256683                0.0                  0.0   \n",
       "24544    0.166468                0.0                  0.0   \n",
       "24545    0.367750                0.0                  0.0   \n",
       "24546    0.533555                0.0                  0.0   \n",
       "24547    0.633596                0.0                  0.0   \n",
       "\n",
       "       Stock_APOLLOHOSP.NS  ...  Stock_SHRIRAMFIN.NS  Stock_SUNPHARMA.NS  \\\n",
       "0                      0.0  ...                  0.0                 0.0   \n",
       "1                      0.0  ...                  0.0                 0.0   \n",
       "2                      0.0  ...                  0.0                 0.0   \n",
       "3                      0.0  ...                  0.0                 0.0   \n",
       "4                      0.0  ...                  0.0                 0.0   \n",
       "...                    ...  ...                  ...                 ...   \n",
       "24543                  0.0  ...                  0.0                 0.0   \n",
       "24544                  0.0  ...                  0.0                 0.0   \n",
       "24545                  0.0  ...                  0.0                 0.0   \n",
       "24546                  0.0  ...                  0.0                 0.0   \n",
       "24547                  0.0  ...                  0.0                 0.0   \n",
       "\n",
       "       Stock_TATACONSUM.NS  Stock_TATAMOTORS.NS  Stock_TATASTEEL.NS  \\\n",
       "0                      0.0                  0.0                 0.0   \n",
       "1                      0.0                  0.0                 0.0   \n",
       "2                      0.0                  0.0                 0.0   \n",
       "3                      0.0                  0.0                 0.0   \n",
       "4                      0.0                  0.0                 0.0   \n",
       "...                    ...                  ...                 ...   \n",
       "24543                  0.0                  0.0                 0.0   \n",
       "24544                  0.0                  0.0                 0.0   \n",
       "24545                  0.0                  0.0                 0.0   \n",
       "24546                  0.0                  0.0                 0.0   \n",
       "24547                  0.0                  0.0                 0.0   \n",
       "\n",
       "       Stock_TCS.NS  Stock_TECHM.NS  Stock_TITAN.NS  Stock_ULTRACEMCO.NS  \\\n",
       "0               0.0             0.0             0.0                  0.0   \n",
       "1               0.0             0.0             0.0                  0.0   \n",
       "2               0.0             0.0             0.0                  0.0   \n",
       "3               0.0             0.0             0.0                  0.0   \n",
       "4               0.0             0.0             0.0                  0.0   \n",
       "...             ...             ...             ...                  ...   \n",
       "24543           0.0             0.0             0.0                  0.0   \n",
       "24544           0.0             0.0             0.0                  0.0   \n",
       "24545           0.0             0.0             0.0                  0.0   \n",
       "24546           0.0             0.0             0.0                  0.0   \n",
       "24547           0.0             0.0             0.0                  0.0   \n",
       "\n",
       "       Stock_WIPRO.NS  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "...               ...  \n",
       "24543             1.0  \n",
       "24544             1.0  \n",
       "24545             1.0  \n",
       "24546             1.0  \n",
       "24547             1.0  \n",
       "\n",
       "[24548 rows x 57 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1718822285136,
     "user": {
      "displayName": "Clyden Pacheco",
      "userId": "07798596330758231105"
     },
     "user_tz": -330
    },
    "id": "Ic3kPfmsEZ_o",
    "outputId": "e411e43d-d44c-4620-9d0b-d4e0156562c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Sentiments'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training.columns[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1718822288152,
     "user": {
      "displayName": "Clyden Pacheco",
      "userId": "07798596330758231105"
     },
     "user_tz": -330
    },
    "id": "vcLgJK2-4yug"
   },
   "outputs": [],
   "source": [
    "# Function to create sequences\n",
    "def singleStepSampler(df, window):\n",
    "\txRes = []\n",
    "\tyRes = []\n",
    "\tfor i in range(0, len(df) - window):\n",
    "\t\tres = []\n",
    "\t\tfor j in range(0, window):\n",
    "\t\t\tr = []\n",
    "\t\t\tfor col in df.columns:\n",
    "\t\t\t\tr.append(df[col][i + j])\n",
    "\t\t\tres.append(r)\n",
    "\t\txRes.append(res)\n",
    "\t\tyRes.append(df[['Open', 'Close', 'High', 'Low', 'Adj Close', 'Volume', 'Sentiments']].iloc[i + window].values)\n",
    "\treturn np.array(xRes), np.array(yRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Omz3OAZs5Apb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24528, 20, 57)\n",
      "(24528, 7)\n",
      "(24528, 20, 57)\n",
      "(24528, 7)\n",
      "(0, 20, 57)\n",
      "(0, 7)\n"
     ]
    }
   ],
   "source": [
    "# Dataset splitting\n",
    "SPLIT = 1.0\n",
    "(xVal, yVal) = singleStepSampler(df_for_training, 20)\n",
    "X_train = xVal[:int(SPLIT * len(xVal))]\n",
    "y_train = yVal[:int(SPLIT * len(yVal))]\n",
    "X_test = xVal[int(SPLIT * len(xVal)):]\n",
    "y_test = yVal[int(SPLIT * len(yVal)):]\n",
    "print(xVal.shape)\n",
    "print(yVal.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1718818528922,
     "user": {
      "displayName": "Clyden Pacheco",
      "userId": "07798596330758231105"
     },
     "user_tz": -330
    },
    "id": "1ubmvlCi6f71",
    "outputId": "0feb7b62-5fef-4880-b983-22f808eb0634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 20, 200)           206400    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 20, 200)           0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 100)               120400    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 707       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 327507 (1.25 MB)\n",
      "Trainable params: 327507 (1.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "\n",
    "multivariate_lstm = keras.Sequential()\n",
    "\n",
    "# First LSTM layer to capture short-term dependencies within the window\n",
    "multivariate_lstm.add(keras.layers.LSTM(200, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "multivariate_lstm.add(keras.layers.Dropout(0.2))  # Dropout for regularization\n",
    "\n",
    "# Second LSTM layer to capture potential hierarchical features\n",
    "multivariate_lstm.add(keras.layers.LSTM(100))  # Experiment with different units here\n",
    "multivariate_lstm.add(keras.layers.Dropout(0.2))  # Dropout for regularization\n",
    "\n",
    "# Dense layer with ReLU activation for non-linearity\n",
    "multivariate_lstm.add(keras.layers.Dense(7, activation='relu'))\n",
    "\n",
    "# Compile the model with Adam optimizer and Mean Squared Error loss\n",
    "multivariate_lstm.compile(loss = 'MeanSquaredError', metrics=['MAE'], optimizer='Adam')\n",
    "\n",
    "# Print model summary\n",
    "multivariate_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2967783,
     "status": "ok",
     "timestamp": 1718821505301,
     "user": {
      "displayName": "Clyden Pacheco",
      "userId": "07798596330758231105"
     },
     "user_tz": -330
    },
    "id": "0UiGrwAB6gnU",
    "outputId": "507089bb-5b68-4892-87a7-df4cc4f568a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Clyden Pacheco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Clyden Pacheco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "767/767 [==============================] - 19s 21ms/step - loss: 0.0087 - MAE: 0.0601\n",
      "Epoch 2/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0050 - MAE: 0.0438\n",
      "Epoch 3/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0043 - MAE: 0.0390\n",
      "Epoch 4/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0040 - MAE: 0.0365\n",
      "Epoch 5/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0038 - MAE: 0.0348\n",
      "Epoch 6/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0037 - MAE: 0.0336\n",
      "Epoch 7/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0035 - MAE: 0.0328\n",
      "Epoch 8/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0035 - MAE: 0.0325\n",
      "Epoch 9/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0035 - MAE: 0.0322\n",
      "Epoch 10/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0034 - MAE: 0.0321\n",
      "Epoch 11/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0034 - MAE: 0.0318\n",
      "Epoch 12/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0033 - MAE: 0.0313\n",
      "Epoch 13/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0033 - MAE: 0.0313\n",
      "Epoch 14/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0033 - MAE: 0.0312\n",
      "Epoch 15/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0032 - MAE: 0.0310\n",
      "Epoch 16/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0032 - MAE: 0.0309\n",
      "Epoch 17/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0031 - MAE: 0.0307\n",
      "Epoch 18/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0031 - MAE: 0.0307\n",
      "Epoch 19/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0031 - MAE: 0.0306\n",
      "Epoch 20/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0032 - MAE: 0.0308\n",
      "Epoch 21/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0031 - MAE: 0.0304\n",
      "Epoch 22/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0031 - MAE: 0.0306\n",
      "Epoch 23/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0030 - MAE: 0.0300\n",
      "Epoch 24/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0030 - MAE: 0.0300\n",
      "Epoch 25/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0031 - MAE: 0.0308\n",
      "Epoch 26/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0030 - MAE: 0.0300\n",
      "Epoch 27/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0029 - MAE: 0.0298\n",
      "Epoch 28/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0029 - MAE: 0.0295\n",
      "Epoch 29/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0030 - MAE: 0.0297\n",
      "Epoch 30/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0029 - MAE: 0.0295\n",
      "Epoch 31/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0029 - MAE: 0.0295\n",
      "Epoch 32/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0029 - MAE: 0.0294\n",
      "Epoch 33/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0029 - MAE: 0.0295\n",
      "Epoch 34/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0028 - MAE: 0.0293\n",
      "Epoch 35/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0028 - MAE: 0.0292\n",
      "Epoch 36/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0030 - MAE: 0.0301\n",
      "Epoch 37/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0028 - MAE: 0.0291\n",
      "Epoch 38/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0288\n",
      "Epoch 39/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0288\n",
      "Epoch 40/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0289\n",
      "Epoch 41/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0288\n",
      "Epoch 42/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0287\n",
      "Epoch 43/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0027 - MAE: 0.0290\n",
      "Epoch 44/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0285\n",
      "Epoch 45/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0288\n",
      "Epoch 46/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0287\n",
      "Epoch 47/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0286\n",
      "Epoch 48/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0283\n",
      "Epoch 49/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0287\n",
      "Epoch 50/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0027 - MAE: 0.0285\n",
      "Epoch 51/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0027 - MAE: 0.0284\n",
      "Epoch 52/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0284\n",
      "Epoch 53/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0287\n",
      "Epoch 54/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0285\n",
      "Epoch 55/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0285\n",
      "Epoch 56/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0027 - MAE: 0.0284\n",
      "Epoch 57/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0026 - MAE: 0.0281\n",
      "Epoch 58/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0026 - MAE: 0.0280\n",
      "Epoch 59/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0026 - MAE: 0.0280\n",
      "Epoch 60/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0026 - MAE: 0.0280\n",
      "Epoch 61/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0026 - MAE: 0.0280\n",
      "Epoch 62/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0282\n",
      "Epoch 63/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0281\n",
      "Epoch 64/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0278\n",
      "Epoch 65/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0280\n",
      "Epoch 66/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0277\n",
      "Epoch 67/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0278\n",
      "Epoch 68/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0280\n",
      "Epoch 69/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0026 - MAE: 0.0280\n",
      "Epoch 70/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0279\n",
      "Epoch 71/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0277\n",
      "Epoch 72/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0278\n",
      "Epoch 73/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0025 - MAE: 0.0277\n",
      "Epoch 74/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0276\n",
      "Epoch 75/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0277\n",
      "Epoch 76/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0277\n",
      "Epoch 77/100\n",
      "767/767 [==============================] - 15s 20ms/step - loss: 0.0025 - MAE: 0.0278\n",
      "Epoch 78/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0282\n",
      "Epoch 79/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0026 - MAE: 0.0280\n",
      "Epoch 80/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0025 - MAE: 0.0275\n",
      "Epoch 81/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0025 - MAE: 0.0274\n",
      "Epoch 82/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0025 - MAE: 0.0276\n",
      "Epoch 83/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0275\n",
      "Epoch 84/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0275\n",
      "Epoch 85/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0025 - MAE: 0.0275\n",
      "Epoch 86/100\n",
      "767/767 [==============================] - 17s 22ms/step - loss: 0.0025 - MAE: 0.0275\n",
      "Epoch 87/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0274\n",
      "Epoch 88/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0025 - MAE: 0.0274\n",
      "Epoch 89/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0024 - MAE: 0.0273\n",
      "Epoch 90/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0024 - MAE: 0.0273\n",
      "Epoch 91/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0024 - MAE: 0.0273\n",
      "Epoch 92/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0024 - MAE: 0.0273\n",
      "Epoch 93/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0025 - MAE: 0.0276\n",
      "Epoch 94/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0024 - MAE: 0.0273\n",
      "Epoch 95/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0024 - MAE: 0.0272\n",
      "Epoch 96/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0024 - MAE: 0.0273\n",
      "Epoch 97/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0024 - MAE: 0.0274\n",
      "Epoch 98/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0024 - MAE: 0.0271\n",
      "Epoch 99/100\n",
      "767/767 [==============================] - 16s 20ms/step - loss: 0.0024 - MAE: 0.0273\n",
      "Epoch 100/100\n",
      "767/767 [==============================] - 16s 21ms/step - loss: 0.0025 - MAE: 0.0276\n"
     ]
    }
   ],
   "source": [
    "# Fitting the data to the model\n",
    "history = multivariate_lstm.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1718815059031,
     "user": {
      "displayName": "Clyden Pacheco",
      "userId": "07798596330758231105"
     },
     "user_tz": -330
    },
    "id": "_5KqDP2QCyYa",
    "outputId": "d94af31e-be9b-4e96-87e2-bdfb303a5bdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clyden Pacheco\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model to a specific path\n",
    "model_path = 'C:/Users/Clyden Pacheco/Documents/Models/multivariate_lstm_model3.h5'\n",
    "multivariate_lstm.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOxk8dG1ZddX05T4+uC8fEM",
   "mount_file_id": "1jTr-zAI3mYWWp4AHGLgLJX4RO9-xNF3y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
